{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998e691e",
   "metadata": {},
   "source": [
    "# Pokemon Feature Engineering and Data Preparation\n",
    "\n",
    "This notebook focuses on feature engineering, data cleaning, feature selection, and preparation of the final dataset for modeling. Includes bootstrapping analysis, information gain calculations, and creation of a modeling-ready dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b792fa1",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ab35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# prevent results from being clipped\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# set up consistent color palette across all notebooks\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "presentation_palette = sns.color_palette(\"Set2\")  # 8 colors available\n",
    "\n",
    "# load the pokemon dataset\n",
    "df = pd.read_csv('Pokemon Database.csv')\n",
    "print(f\"dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# define base features\n",
    "base_stats = ['Health Stat', 'Attack Stat', 'Defense Stat', 'Special Attack Stat', 'Special Defense Stat', 'Speed Stat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376c140",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26507e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features for modeling\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# Separate single-type and dual-type Pokemon for analysis\n",
    "single_type = df_engineered[df_engineered['Secondary Type'].isna()]\n",
    "dual_type = df_engineered[~df_engineered['Secondary Type'].isna()]\n",
    "\n",
    "print(f\"Single-type Pokemon: {len(single_type)}\")\n",
    "print(f\"Dual-type Pokemon: {len(dual_type)}\")\n",
    "print()\n",
    "\n",
    "# 1. Stat ratios and differences\n",
    "df_engineered['Physical_Special_Ratio'] = (df_engineered['Attack Stat'] + df_engineered['Defense Stat']) / (df_engineered['Special Attack Stat'] + df_engineered['Special Defense Stat'] + 1)  # +1 to avoid division by zero\n",
    "df_engineered['Offensive_Defensive_Ratio'] = (df_engineered['Attack Stat'] + df_engineered['Special Attack Stat']) / (df_engineered['Defense Stat'] + df_engineered['Special Defense Stat'] + 1)\n",
    "df_engineered['Speed_Agility_Index'] = df_engineered['Speed Stat'] / (df_engineered['Health Stat'] + 1)\n",
    "\n",
    "# 2. Stat categories (high/medium/low)\n",
    "for stat in base_stats:\n",
    "    df_engineered[f'{stat[:-5]}_Category'] = pd.qcut(df_engineered[stat], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# 3. Type combination features for dual-type Pokemon\n",
    "df_engineered['Type_Combination'] = df_engineered.apply(\n",
    "    lambda row: f\"{row['Primary Type']}+{row['Secondary Type']}\" if not pd.isna(row['Secondary Type']) else row['Primary Type'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 4. Type effectiveness indicators (simplified)\n",
    "type_effectiveness = {\n",
    "    'Fire': ['Grass', 'Bug', 'Ice', 'Steel'],\n",
    "    'Water': ['Fire', 'Ground', 'Rock'],\n",
    "    'Grass': ['Water', 'Ground', 'Rock'],\n",
    "    'Electric': ['Water', 'Flying'],\n",
    "    'Ice': ['Grass', 'Ground', 'Flying', 'Dragon'],\n",
    "    'Fighting': ['Normal', 'Ice', 'Rock', 'Dark', 'Steel'],\n",
    "    'Poison': ['Grass', 'Fairy'],\n",
    "    'Ground': ['Electric', 'Rock', 'Poison', 'Fire', 'Steel'],\n",
    "    'Flying': ['Grass', 'Fighting', 'Bug'],\n",
    "    'Psychic': ['Fighting', 'Poison'],\n",
    "    'Bug': ['Grass', 'Psychic', 'Dark'],\n",
    "    'Rock': ['Flying', 'Bug', 'Ice', 'Fire'],\n",
    "    'Ghost': ['Psychic', 'Ghost'],\n",
    "    'Dragon': ['Dragon'],\n",
    "    'Dark': ['Psychic', 'Ghost'],\n",
    "    'Steel': ['Ice', 'Rock', 'Fairy'],\n",
    "    'Fairy': ['Fighting', 'Dragon', 'Dark']\n",
    "}\n",
    "\n",
    "# Create effectiveness score for primary type\n",
    "df_engineered['Primary_Type_Effectiveness'] = df_engineered['Primary Type'].map(lambda x: len(type_effectiveness.get(x, [])))\n",
    "\n",
    "# Create effectiveness score for secondary type (0 if single type)\n",
    "df_engineered['Secondary_Type_Effectiveness'] = df_engineered['Secondary Type'].map(lambda x: len(type_effectiveness.get(x, [])) if not pd.isna(x) else 0)\n",
    "\n",
    "# Combined effectiveness for dual types\n",
    "df_engineered['Combined_Type_Effectiveness'] = df_engineered['Primary_Type_Effectiveness'] + df_engineered['Secondary_Type_Effectiveness']\n",
    "\n",
    "# 5. Legendary indicators\n",
    "df_engineered['High_Stat_Count'] = (df_engineered[base_stats] > df_engineered[base_stats].quantile(0.8)).sum(axis=1)\n",
    "df_engineered['Stat_Variance'] = df_engineered[base_stats].var(axis=1)\n",
    "\n",
    "# 6. Type diversity features\n",
    "df_engineered['Is_Single_Type'] = df_engineered['Secondary Type'].isna().astype(int)\n",
    "df_engineered['Is_Dual_Type'] = (~df_engineered['Secondary Type'].isna()).astype(int)\n",
    "\n",
    "print(f\"Original features: {len(df.columns)}\")\n",
    "print(f\"Engineered features: {len(df_engineered.columns)}\")\n",
    "print(f\"New features added: {len(df_engineered.columns) - len(df.columns)}\")\n",
    "\n",
    "# Show new features\n",
    "new_features = [col for col in df_engineered.columns if col not in df.columns]\n",
    "print(f\"\\nNew features created: {new_features}\")\n",
    "\n",
    "# Summary of type combinations\n",
    "print(f\"\\n=== TYPE COMBINATION SUMMARY ===\")\n",
    "print(f\"Unique single types: {single_type['Primary Type'].nunique()}\")\n",
    "print(f\"Unique dual-type combinations: {dual_type['Type_Combination'].nunique()}\")\n",
    "print(f\"Most common type combination: {dual_type['Type_Combination'].value_counts().index[0]} ({dual_type['Type_Combination'].value_counts().iloc[0]} Pokemon)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294b331",
   "metadata": {},
   "source": [
    "## Feature Selection and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for feature selection\n",
    "# encode categorical variables\n",
    "le_type = LabelEncoder()\n",
    "le_combo = LabelEncoder()\n",
    "\n",
    "df_encoded = df_engineered.copy()\n",
    "df_encoded['Primary_Type_Encoded'] = le_type.fit_transform(df_encoded['Primary Type'])\n",
    "df_encoded['Type_Combination_Encoded'] = le_combo.fit_transform(df_encoded['Type_Combination'])\n",
    "\n",
    "# separate datasets for single-type and dual-type analysis\n",
    "single_encoded = df_encoded[df_encoded['Is_Single_Type'] == 1]\n",
    "dual_encoded = df_encoded[df_encoded['Is_Dual_Type'] == 1]\n",
    "\n",
    "# select numeric features for analysis\n",
    "numeric_features = df_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features = [f for f in numeric_features if f not in ['Primary_Type_Encoded', 'Type_Combination_Encoded']]  # remove targets\n",
    "\n",
    "print(f\"total numeric features available: {len(numeric_features)}\")\n",
    "print(f\"single-type pokemon: {len(single_encoded)}\")\n",
    "print(f\"dual-type pokemon: {len(dual_encoded)}\")\n",
    "\n",
    "# 1. feature selection for primary type classification (all pokemon)\n",
    "print(\"feature selection for primary type prediction (all pokemon)\")\n",
    "\n",
    "X_type_all = df_encoded[numeric_features]\n",
    "y_type_all = df_encoded['Primary_Type_Encoded']\n",
    "\n",
    "selector_f_type_all = SelectKBest(score_func=f_classif, k='all')\n",
    "selector_f_type_all.fit(X_type_all, y_type_all)\n",
    "top_f_type_all = pd.DataFrame({\n",
    "    \"feature\": X_type_all.columns,\n",
    "    \"score\": selector_f_type_all.scores_\n",
    "}).sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "selector_mi_type_all = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "selector_mi_type_all.fit(X_type_all, y_type_all)\n",
    "top_mi_type_all = pd.DataFrame({\n",
    "    \"feature\": X_type_all.columns,\n",
    "    \"score\": selector_mi_type_all.scores_\n",
    "}).sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# 2. feature selection for primary type (single-type pokemon only)\n",
    "print(\"\\nfeature selection for primary type prediction (single-type only)\")\n",
    "\n",
    "X_type_single = single_encoded[numeric_features]\n",
    "y_type_single = single_encoded['Primary_Type_Encoded']\n",
    "\n",
    "selector_f_type_single = SelectKBest(score_func=f_classif, k='all')\n",
    "selector_f_type_single.fit(X_type_single, y_type_single)\n",
    "top_f_type_single = pd.DataFrame({\n",
    "    \"feature\": X_type_single.columns,\n",
    "    \"score\": selector_f_type_single.scores_\n",
    "}).sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# 3. feature selection for type combination (dual-type pokemon only)\n",
    "print(\"\\nfeature selection for type combination prediction (dual-type only)\")\n",
    "\n",
    "X_combo_dual = dual_encoded[numeric_features]\n",
    "y_combo_dual = dual_encoded['Type_Combination_Encoded']\n",
    "\n",
    "selector_f_combo_dual = SelectKBest(score_func=f_classif, k='all')\n",
    "selector_f_combo_dual.fit(X_combo_dual, y_combo_dual)\n",
    "top_f_combo_dual = pd.DataFrame({\n",
    "    \"feature\": X_combo_dual.columns,\n",
    "    \"score\": selector_f_combo_dual.scores_\n",
    "}).sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# determine thresholds\n",
    "f_threshold_type_all = top_f_type_all['score'].quantile(0.75)\n",
    "f_threshold_type_single = top_f_type_single['score'].quantile(0.75)\n",
    "f_threshold_combo_dual = top_f_combo_dual['score'].quantile(0.75)\n",
    "\n",
    "combined_threshold = min(f_threshold_type_all, f_threshold_type_single, f_threshold_combo_dual)\n",
    "\n",
    "# display thresholds as table\n",
    "thresholds_comparison = pd.DataFrame({\n",
    "    'analysis type': ['all pokemon (primary type)', 'single-type only (primary type)', 'dual-type only (combinations)'],\n",
    "    'threshold (75th percentile)': [f_threshold_type_all, f_threshold_type_single, f_threshold_combo_dual],\n",
    "    'features above threshold': [\n",
    "        len(top_f_type_all[top_f_type_all['score'] >= combined_threshold]),\n",
    "        len(top_f_type_single[top_f_type_single['score'] >= combined_threshold]),\n",
    "        len(top_f_combo_dual[top_f_combo_dual['score'] >= combined_threshold])\n",
    "    ]\n",
    "}).round(4)\n",
    "print(\"\\nfeature selection thresholds comparison\")\n",
    "thresholds_comparison\n",
    "\n",
    "# 4. information gain for legendary classification\n",
    "legendary_features_excl_target = [f for f in numeric_features if f != 'Is Legendary']\n",
    "X_legendary = df_encoded[legendary_features_excl_target]\n",
    "y_legendary = df_encoded['Is Legendary']\n",
    "\n",
    "# feature selection for legendary prediction\n",
    "print(\"\\nfeature selection for legendary prediction\")\n",
    "\n",
    "# f-classif\n",
    "selector_f_legendary = SelectKBest(score_func=f_classif, k='all')\n",
    "selector_f_legendary.fit(X_legendary, y_legendary)\n",
    "top_f_legendary = pd.DataFrame({\n",
    "    \"feature\": X_legendary.columns,\n",
    "    \"score\": selector_f_legendary.scores_\n",
    "}).sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# mutual info\n",
    "selector_mi_legendary = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "selector_mi_legendary.fit(X_legendary, y_legendary)\n",
    "top_mi_legendary = pd.DataFrame({\n",
    "    \"feature\": X_legendary.columns,\n",
    "    \"score\": selector_mi_legendary.scores_\n",
    "}).sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# determine threshold for legendary prediction\n",
    "f_threshold_legendary = top_f_legendary['score'].quantile(0.75)\n",
    "mi_threshold_legendary = top_mi_legendary['score'].quantile(0.75)\n",
    "combined_threshold_legendary = min(f_threshold_legendary, mi_threshold_legendary)\n",
    "\n",
    "# display thresholds as table\n",
    "thresholds_legendary = pd.DataFrame({\n",
    "    'method': ['f-classif', 'mutual info', 'combined'],\n",
    "    'threshold (75th percentile)': [f_threshold_legendary, mi_threshold_legendary, combined_threshold_legendary]\n",
    "}).round(4)\n",
    "print(\"\\nfeature selection thresholds for legendary prediction\")\n",
    "thresholds_legendary\n",
    "\n",
    "# select features above threshold in both methods\n",
    "selected_f_legendary = set(top_f_legendary[top_f_legendary['score'] >= combined_threshold_legendary]['feature'])\n",
    "selected_mi_legendary = set(top_mi_legendary[top_mi_legendary['score'] >= combined_threshold_legendary]['feature'])\n",
    "selected_legendary_features = list(selected_f_legendary & selected_mi_legendary)\n",
    "\n",
    "# display feature selection results as table\n",
    "feature_selection_legendary = pd.DataFrame({\n",
    "    'selection method': ['f-classif above threshold', 'mutual info above threshold', 'both methods (intersection)'],\n",
    "    'features selected': [len(selected_f_legendary), len(selected_mi_legendary), len(selected_legendary_features)],\n",
    "    'feature names': [', '.join(sorted(list(selected_f_legendary))), ', '.join(sorted(list(selected_mi_legendary))), ', '.join(sorted(selected_legendary_features))]\n",
    "})\n",
    "print(\"\\nfeature selection results for legendary prediction\")\n",
    "feature_selection_legendary\n",
    "\n",
    "# visualization of feature importance across different analyses\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 16))\n",
    "\n",
    "# all pokemon - primary type (f-classif)\n",
    "top_f_type_all_plot = top_f_type_all.head(15)\n",
    "sns.barplot(x=\"score\", y=\"feature\", data=top_f_type_all_plot, ax=axs[0,0], color=presentation_palette[0])\n",
    "axs[0,0].set_title(f\"top features - all pokemon\\nprimary type prediction (f-classif)\")\n",
    "\n",
    "# single-type only - primary type (f-classif)\n",
    "top_f_type_single_plot = top_f_type_single.head(15)\n",
    "sns.barplot(x=\"score\", y=\"feature\", data=top_f_type_single_plot, ax=axs[0,1], color=presentation_palette[1])\n",
    "axs[0,1].set_title(f\"top features - single-type only\\nprimary type prediction\")\n",
    "\n",
    "# dual-type only - combinations (f-classif)\n",
    "top_f_combo_dual_plot = top_f_combo_dual.head(15)\n",
    "sns.barplot(x=\"score\", y=\"feature\", data=top_f_combo_dual_plot, ax=axs[0,2], color=presentation_palette[2])\n",
    "axs[0,2].set_title(f\"top features - dual-type only\\ntype combination prediction\")\n",
    "\n",
    "# legendary prediction f-classif\n",
    "top_f_legendary_plot = top_f_legendary.head(15)\n",
    "sns.barplot(x=\"score\", y=\"feature\", data=top_f_legendary_plot, ax=axs[1,0], color=presentation_palette[3])\n",
    "axs[1,0].set_title(f\"top features - legendary prediction\\n(f-classif)\")\n",
    "\n",
    "# legendary prediction mutual info\n",
    "top_mi_legendary_plot = top_mi_legendary.head(15)\n",
    "sns.barplot(x=\"score\", y=\"feature\", data=top_mi_legendary_plot, ax=axs[1,1], color=presentation_palette[4])\n",
    "axs[1,1].set_title(f\"top features - legendary prediction\\n(mutual info)\")\n",
    "\n",
    "# feature comparison across analyses\n",
    "axs[1,2].axis('off')  # turn off the last subplot\n",
    "\n",
    "# add summary text\n",
    "summary_text = f\"\"\"\n",
    "feature engineering summary:\n",
    "\n",
    "• total pokemon: {len(df_encoded)}\n",
    "• single-type: {len(single_encoded)} ({len(single_encoded)/len(df_encoded)*100:.1f}%)\n",
    "• dual-type: {len(dual_encoded)} ({len(dual_encoded)/len(df_encoded)*100:.1f}%)\n",
    "\n",
    "• features engineered: {len(new_features)}\n",
    "• type combinations identified: {dual_encoded['Type_Combination'].nunique()}\n",
    "\n",
    "• primary type prediction features: {len(selected_type_features) if 'selected_type_features' in locals() else 'tbd'}\n",
    "• legendary prediction features: {len(selected_legendary_features)}\n",
    "\"\"\"\n",
    "\n",
    "axs[1,2].text(0.1, 0.8, summary_text, transform=axs[1,2].transAxes,\n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.suptitle(\"feature importance across different pokemon type prediction tasks\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513d6cc",
   "metadata": {},
   "source": [
    "## Bootstrapping Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f22ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping analysis for feature stability\n",
    "def bootstrap_feature_importance(X, y, n_bootstraps=100, random_state=42):\n",
    "    \"\"\"calculate feature importance using bootstrapping\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    feature_names = X.columns\n",
    "    n_features = len(feature_names)\n",
    "\n",
    "    # store importance scores for each bootstrap\n",
    "    importance_scores = np.zeros((n_bootstraps, n_features))\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap sample\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        X_boot = X.iloc[indices]\n",
    "        y_boot = y.iloc[indices]\n",
    "\n",
    "        # train model and get feature importance\n",
    "        rf = RandomForestClassifier(n_estimators=50, random_state=i, max_depth=10)\n",
    "        rf.fit(X_boot, y_boot)\n",
    "        importance_scores[i] = rf.feature_importances_\n",
    "\n",
    "    # calculate statistics\n",
    "    mean_importance = np.mean(importance_scores, axis=0)\n",
    "    std_importance = np.std(importance_scores, axis=0)\n",
    "    ci_lower = np.percentile(importance_scores, 2.5, axis=0)\n",
    "    ci_upper = np.percentile(importance_scores, 97.5, axis=0)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean_importance': mean_importance,\n",
    "        'std_importance': std_importance,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'stability': mean_importance / std_importance  # coefficient of variation\n",
    "    }).sort_values('mean_importance', ascending=False)\n",
    "\n",
    "# bootstrap analysis for type prediction\n",
    "print(\"running bootstrap analysis for type prediction (this may take a moment)...\")\n",
    "bootstrap_type = bootstrap_feature_importance(X_type, pd.Series(y_type), n_bootstraps=50)\n",
    "\n",
    "print(\"\\nbootstrap feature importance for type prediction\")\n",
    "print(\"top 10 most stable features:\")\n",
    "bootstrap_type.head(10)[['feature', 'mean_importance', 'std_importance', 'stability']].round(4)\n",
    "\n",
    "# bootstrap analysis for legendary prediction\n",
    "print(\"\\nrunning bootstrap analysis for legendary prediction...\")\n",
    "bootstrap_legendary = bootstrap_feature_importance(X_legendary, y_legendary, n_bootstraps=50)\n",
    "\n",
    "print(\"\\nbootstrap feature importance for legendary prediction\")\n",
    "print(\"top 10 most stable features:\")\n",
    "bootstrap_legendary.head(10)[['feature', 'mean_importance', 'std_importance', 'stability']].round(4)\n",
    "\n",
    "# visualize bootstrap results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# type prediction bootstrap\n",
    "top_bootstrap_type = bootstrap_type.head(8)\n",
    "x_pos = np.arange(len(top_bootstrap_type))\n",
    "axes[0].bar(x_pos, top_bootstrap_type['mean_importance'], yerr=top_bootstrap_type['std_importance'],\n",
    "           color=presentation_palette[0], alpha=0.7, capsize=5)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels([f.replace('_', '\\n') for f in top_bootstrap_type['feature']], rotation=45, ha='right', fontsize=8)\n",
    "axes[0].set_title('bootstrap feature importance\\n(type prediction)', fontweight='bold')\n",
    "axes[0].set_ylabel('importance score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# legendary prediction bootstrap\n",
    "top_bootstrap_leg = bootstrap_legendary.head(8)\n",
    "x_pos = np.arange(len(top_bootstrap_leg))\n",
    "axes[1].bar(x_pos, top_bootstrap_leg['mean_importance'], yerr=top_bootstrap_leg['std_importance'],\n",
    "           color=presentation_palette[1], alpha=0.7, capsize=5)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([f.replace('_', '\\n') for f in top_bootstrap_leg['feature']], rotation=45, ha='right', fontsize=8)\n",
    "axes[1].set_title('bootstrap feature importance\\n(legendary prediction)', fontweight='bold')\n",
    "axes[1].set_ylabel('importance score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/bootstrap_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf31ca2",
   "metadata": {},
   "source": [
    "## Final Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205cf3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final modeling datasets\n",
    "\n",
    "# 1. Single-Type Primary Type Classification Dataset\n",
    "single_type_features = list(set(top_f_type_single[top_f_type_single['Score'] >= combined_threshold]['Feature']))\n",
    "df_single_type_modeling = single_encoded[single_type_features + ['Primary Type', 'Primary_Type_Encoded']].copy()\n",
    "\n",
    "# 2. Dual-Type Combination Classification Dataset\n",
    "dual_combo_features = list(set(top_f_combo_dual[top_f_combo_dual['Score'] >= combined_threshold]['Feature']))\n",
    "df_dual_combo_modeling = dual_encoded[dual_combo_features + ['Type_Combination', 'Type_Combination_Encoded']].copy()\n",
    "\n",
    "# 3. All Pokemon Primary Type Classification Dataset\n",
    "all_type_features = list(set(top_f_type_all[top_f_type_all['Score'] >= combined_threshold]['Feature']))\n",
    "df_all_type_modeling = df_encoded[all_type_features + ['Primary Type', 'Primary_Type_Encoded']].copy()\n",
    "\n",
    "# 4. Legendary Classification Dataset\n",
    "legendary_features = selected_legendary_features\n",
    "df_legendary_modeling = df_encoded[legendary_features + ['Is Legendary']].copy()\n",
    "\n",
    "# 5. Clustering Dataset (unsupervised)\n",
    "clustering_features = [f for f in numeric_features if f not in ['Primary_Type_Encoded', 'Type_Combination_Encoded', 'Is Legendary', 'Is_Single_Type', 'Is_Dual_Type']]\n",
    "df_clustering = df_encoded[clustering_features].copy()\n",
    "\n",
    "# Display comprehensive dataset summaries\n",
    "dataset_summary = pd.DataFrame({\n",
    "    'Dataset': [\n",
    "        'Single-Type Primary Classification',\n",
    "        'Dual-Type Combination Classification',\n",
    "        'All Pokemon Primary Classification',\n",
    "        'Legendary Classification',\n",
    "        'Clustering (Unsupervised)'\n",
    "    ],\n",
    "    'Samples': [\n",
    "        df_single_type_modeling.shape[0],\n",
    "        df_dual_combo_modeling.shape[0],\n",
    "        df_all_type_modeling.shape[0],\n",
    "        df_legendary_modeling.shape[0],\n",
    "        df_clustering.shape[0]\n",
    "    ],\n",
    "    'Features': [\n",
    "        len(single_type_features),\n",
    "        len(dual_combo_features),\n",
    "        len(all_type_features),\n",
    "        len(legendary_features),\n",
    "        len(clustering_features)\n",
    "    ],\n",
    "    'Target Variable': [\n",
    "        'Primary Type',\n",
    "        'Type Combination',\n",
    "        'Primary Type',\n",
    "        'Is Legendary',\n",
    "        'None (unsupervised)'\n",
    "    ],\n",
    "    'Pokemon Types Covered': [\n",
    "        f'{single_encoded[\"Primary Type\"].nunique()} single types',\n",
    "        f'{dual_encoded[\"Type_Combination\"].nunique()} combinations',\n",
    "        f'{df_encoded[\"Primary Type\"].nunique()} primary types',\n",
    "        'All Pokemon',\n",
    "        'All Pokemon'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== COMPREHENSIVE MODELING DATASETS CREATED ===\")\n",
    "print(f\"Total engineered features: {len([col for col in df_engineered.columns if col not in df.columns])}\")\n",
    "print(f\"Type combinations identified: {dual_encoded['Type_Combination'].nunique()}\")\n",
    "print(f\"Single-type Pokemon: {len(single_encoded)} ({len(single_encoded)/len(df_encoded)*100:.1f}%)\")\n",
    "print(f\"Dual-type Pokemon: {len(dual_encoded)} ({len(dual_encoded)/len(df_encoded)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "dataset_summary\n",
    "\n",
    "# Save the engineered dataset for use in modeling\n",
    "df_engineered.to_csv('pokemon_engineered.csv', index=False)\n",
    "print(\"\\n=== ENGINEERED DATASET SAVED ===\")\n",
    "print(\"File: pokemon_engineered.csv\")\n",
    "print(f\"Shape: {df_engineered.shape}\")\n",
    "print(f\"Features: {list(df_engineered.columns)}\")\n",
    "\n",
    "# Scale the datasets\n",
    "scaler_type = StandardScaler()\n",
    "scaler_legendary = StandardScaler()\n",
    "scaler_clustering = StandardScaler()\n",
    "\n",
    "# Scale type dataset\n",
    "numeric_type_features = [f for f in type_features if df_type_modeling[f].dtype in ['int64', 'float64']]\n",
    "df_type_modeling_scaled = df_type_modeling.copy()\n",
    "df_type_modeling_scaled[numeric_type_features] = scaler_type.fit_transform(df_type_modeling[numeric_type_features])\n",
    "\n",
    "# Scale legendary dataset\n",
    "numeric_legendary_features = [f for f in legendary_features if df_legendary_modeling[f].dtype in ['int64', 'float64']]\n",
    "df_legendary_modeling_scaled = df_legendary_modeling.copy()\n",
    "df_legendary_modeling_scaled[numeric_legendary_features] = scaler_legendary.fit_transform(df_legendary_modeling[numeric_legendary_features])\n",
    "\n",
    "# Scale clustering dataset\n",
    "df_clustering_scaled = pd.DataFrame(\n",
    "    scaler_clustering.fit_transform(df_clustering),\n",
    "    columns=df_clustering.columns,\n",
    "    index=df_clustering.index\n",
    ")\n",
    "\n",
    "# Save final datasets\n",
    "df_type_modeling_scaled.to_csv('pokemon_type_modeling.csv', index=False)\n",
    "df_legendary_modeling_scaled.to_csv('pokemon_legendary_modeling.csv', index=False)\n",
    "df_clustering_scaled.to_csv('pokemon_clustering.csv', index=False)\n",
    "\n",
    "# Dataset files summary\n",
    "dataset_files = pd.DataFrame({\n",
    "    'File': ['pokemon_type_modeling.csv', 'pokemon_legendary_modeling.csv', 'pokemon_clustering.csv'],\n",
    "    'Purpose': ['Type classification', 'Legendary classification', 'Clustering analysis'],\n",
    "    'Features': [len(type_features), len(legendary_features), len(clustering_features)],\n",
    "    'Target': ['Primary_Type_Encoded', 'Is Legendary', 'None (unsupervised)']\n",
    "})\n",
    "\n",
    "print(\"\\n=== DATASET FILES SAVED ===\")\n",
    "dataset_files\n",
    "\n",
    "# Quick validation - test model performance with selected features\n",
    "print(\"\\n=== MODEL VALIDATION WITH SELECTED FEATURES ===\")\n",
    "\n",
    "# Type classification quick test\n",
    "X_type_sel = df_type_modeling_scaled[numeric_type_features]\n",
    "y_type_sel = df_type_modeling_scaled['Primary_Type_Encoded']\n",
    "\n",
    "X_train_type, X_test_type, y_train_type, y_test_type = train_test_split(\n",
    "    X_type_sel, y_type_sel, test_size=0.2, random_state=42, stratify=y_type_sel\n",
    ")\n",
    "\n",
    "rf_type = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_type.fit(X_train_type, y_train_type)\n",
    "type_accuracy = rf_type.score(X_test_type, y_test_type)\n",
    "\n",
    "# Legendary classification quick test\n",
    "X_leg_sel = df_legendary_modeling_scaled[numeric_legendary_features]\n",
    "y_leg_sel = df_legendary_modeling_scaled['Is Legendary']\n",
    "\n",
    "X_train_leg, X_test_leg, y_train_leg, y_test_leg = train_test_split(\n",
    "    X_leg_sel, y_leg_sel, test_size=0.2, random_state=42, stratify=y_leg_sel\n",
    ")\n",
    "\n",
    "rf_leg = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_leg.fit(X_train_leg, y_train_leg)\n",
    "legendary_accuracy = rf_leg.score(X_test_leg, y_test_leg)\n",
    "\n",
    "# Display validation results as table\n",
    "validation_results = pd.DataFrame({\n",
    "    'Task': ['Type Classification', 'Legendary Classification'],\n",
    "    'Test Accuracy': [f'{type_accuracy:.3f}', f'{legendary_accuracy:.3f}'],\n",
    "    'Features Used': [len(numeric_type_features), len(numeric_legendary_features)],\n",
    "    'Model': ['Random Forest (100 trees)', 'Random Forest (100 trees)']\n",
    "})\n",
    "validation_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
